{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"15HEsshJdacJ4obF-w2r9ygP-G0RKzGz3","timestamp":1682576318780}],"authorship_tag":"ABX9TyOu6Vh+QgVltYLu0L/G0FBb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OoJBz5zwL3o4","executionInfo":{"status":"ok","timestamp":1682573367444,"user_tz":-330,"elapsed":3672,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}},"outputId":"6188df2a-8864-41e3-b610-2f64e2c1a278"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","source":["text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\"\n","from nltk.tokenize import sent_tokenize\n","tokenized_text=sent_tokenize(text)\n","tokenized_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J9qPixAPMKiG","executionInfo":{"status":"ok","timestamp":1682573411078,"user_tz":-330,"elapsed":530,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}},"outputId":"0c5c5f36-de7a-4e20-97a1-a5eb0d199a12"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Tokenization is the first step in text analytics.',\n"," 'The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.']"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","tokenized_word=word_tokenize(text)\n","tokenized_word\n","     "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ihK8FnW6MPrh","executionInfo":{"status":"ok","timestamp":1682573431467,"user_tz":-330,"elapsed":14,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}},"outputId":"2d75c739-faf5-4200-e78b-52d5b07adb41"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Tokenization',\n"," 'is',\n"," 'the',\n"," 'first',\n"," 'step',\n"," 'in',\n"," 'text',\n"," 'analytics',\n"," '.',\n"," 'The',\n"," 'process',\n"," 'of',\n"," 'breaking',\n"," 'down',\n"," 'a',\n"," 'text',\n"," 'paragraph',\n"," 'into',\n"," 'smaller',\n"," 'chunks',\n"," 'such',\n"," 'as',\n"," 'words',\n"," 'or',\n"," 'sentences',\n"," 'is',\n"," 'called',\n"," 'Tokenization',\n"," '.']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","stop_words =set(stopwords.words(\"english\"))"],"metadata":{"id":"5uo8tje1MU-f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stop_words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MMx1ixULMYdB","executionInfo":{"status":"ok","timestamp":1682573465760,"user_tz":-330,"elapsed":16,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}},"outputId":"412cc843-bfb7-4cbe-f132-5e2dd4e8113b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'a',\n"," 'about',\n"," 'above',\n"," 'after',\n"," 'again',\n"," 'against',\n"," 'ain',\n"," 'all',\n"," 'am',\n"," 'an',\n"," 'and',\n"," 'any',\n"," 'are',\n"," 'aren',\n"," \"aren't\",\n"," 'as',\n"," 'at',\n"," 'be',\n"," 'because',\n"," 'been',\n"," 'before',\n"," 'being',\n"," 'below',\n"," 'between',\n"," 'both',\n"," 'but',\n"," 'by',\n"," 'can',\n"," 'couldn',\n"," \"couldn't\",\n"," 'd',\n"," 'did',\n"," 'didn',\n"," \"didn't\",\n"," 'do',\n"," 'does',\n"," 'doesn',\n"," \"doesn't\",\n"," 'doing',\n"," 'don',\n"," \"don't\",\n"," 'down',\n"," 'during',\n"," 'each',\n"," 'few',\n"," 'for',\n"," 'from',\n"," 'further',\n"," 'had',\n"," 'hadn',\n"," \"hadn't\",\n"," 'has',\n"," 'hasn',\n"," \"hasn't\",\n"," 'have',\n"," 'haven',\n"," \"haven't\",\n"," 'having',\n"," 'he',\n"," 'her',\n"," 'here',\n"," 'hers',\n"," 'herself',\n"," 'him',\n"," 'himself',\n"," 'his',\n"," 'how',\n"," 'i',\n"," 'if',\n"," 'in',\n"," 'into',\n"," 'is',\n"," 'isn',\n"," \"isn't\",\n"," 'it',\n"," \"it's\",\n"," 'its',\n"," 'itself',\n"," 'just',\n"," 'll',\n"," 'm',\n"," 'ma',\n"," 'me',\n"," 'mightn',\n"," \"mightn't\",\n"," 'more',\n"," 'most',\n"," 'mustn',\n"," \"mustn't\",\n"," 'my',\n"," 'myself',\n"," 'needn',\n"," \"needn't\",\n"," 'no',\n"," 'nor',\n"," 'not',\n"," 'now',\n"," 'o',\n"," 'of',\n"," 'off',\n"," 'on',\n"," 'once',\n"," 'only',\n"," 'or',\n"," 'other',\n"," 'our',\n"," 'ours',\n"," 'ourselves',\n"," 'out',\n"," 'over',\n"," 'own',\n"," 're',\n"," 's',\n"," 'same',\n"," 'shan',\n"," \"shan't\",\n"," 'she',\n"," \"she's\",\n"," 'should',\n"," \"should've\",\n"," 'shouldn',\n"," \"shouldn't\",\n"," 'so',\n"," 'some',\n"," 'such',\n"," 't',\n"," 'than',\n"," 'that',\n"," \"that'll\",\n"," 'the',\n"," 'their',\n"," 'theirs',\n"," 'them',\n"," 'themselves',\n"," 'then',\n"," 'there',\n"," 'these',\n"," 'they',\n"," 'this',\n"," 'those',\n"," 'through',\n"," 'to',\n"," 'too',\n"," 'under',\n"," 'until',\n"," 'up',\n"," 've',\n"," 'very',\n"," 'was',\n"," 'wasn',\n"," \"wasn't\",\n"," 'we',\n"," 'were',\n"," 'weren',\n"," \"weren't\",\n"," 'what',\n"," 'when',\n"," 'where',\n"," 'which',\n"," 'while',\n"," 'who',\n"," 'whom',\n"," 'why',\n"," 'will',\n"," 'with',\n"," 'won',\n"," \"won't\",\n"," 'wouldn',\n"," \"wouldn't\",\n"," 'y',\n"," 'you',\n"," \"you'd\",\n"," \"you'll\",\n"," \"you're\",\n"," \"you've\",\n"," 'your',\n"," 'yours',\n"," 'yourself',\n"," 'yourselves'}"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["text= \"How to remove stop words with NLTK library in Python?\"\n","import re\n","text= re.sub('[^a-zA-Z]', ' ',text)\n","text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"2vTU0vAOMcv9","executionInfo":{"status":"ok","timestamp":1682573484044,"user_tz":-330,"elapsed":22,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}},"outputId":"591940af-9e8c-4a11-b108-e58a81ffc2a0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'How to remove stop words with NLTK library in Python '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["tokens=word_tokenize(text.lower())\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pmxfuGqMMgcM","executionInfo":{"status":"ok","timestamp":1682573499042,"user_tz":-330,"elapsed":13,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}},"outputId":"507a6d6e-4745-46c4-a0f3-bdf834f54ae6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['how',\n"," 'to',\n"," 'remove',\n"," 'stop',\n"," 'words',\n"," 'with',\n"," 'nltk',\n"," 'library',\n"," 'in',\n"," 'python']"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["filtered_text=[]\n","for w in tokens:\n","   if w not in stop_words:\n","      filtered_text.append(w)\n","     "],"metadata":{"id":"CLG9UBpWMjdS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","filtered_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cntMkjqvMm2U","executionInfo":{"status":"ok","timestamp":1682573524741,"user_tz":-330,"elapsed":12,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}},"outputId":"589ed1ae-bb04-42b1-b2bb-2a5000041ab3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['remove', 'stop', 'words', 'nltk', 'library', 'python']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n","ps =PorterStemmer()\n","for w in e_words:\n","  rootWord=ps.stem(w)\n","  print(rootWord)  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FbFi8CFfMp0M","executionInfo":{"status":"ok","timestamp":1682573538059,"user_tz":-330,"elapsed":636,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}},"outputId":"bb59b8a0-cfa5-4337-aa7a-dc8eb55a5533"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["wait\n","wait\n","wait\n","wait\n"]}]},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","text = \"studies study cries cry\"\n","tokenization = nltk.word_tokenize(text)\n","for w in tokenization:\n","    print(\"Lemma for {} is {}\".format(w,wordnet_lemmatizer.lemmatize(w)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4CBwD_FFM7W-","executionInfo":{"status":"ok","timestamp":1682573627751,"user_tz":-330,"elapsed":728,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}},"outputId":"99d9842a-803c-4707-c064-e78ad704ba6c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Lemma for studies is study\n","Lemma for study is study\n","Lemma for cries is cry\n","Lemma for cry is cry\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","data=\"The pink sweater fit her perfectly\"\n","words=word_tokenize(data)\n","for word in words:\n","    print(nltk.pos_tag([word]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F7M5Bhp6NEGB","executionInfo":{"status":"ok","timestamp":1682573644544,"user_tz":-330,"elapsed":12,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}},"outputId":"8d660379-fc05-41f9-9af6-bffe90dd030b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('The', 'DT')]\n","[('pink', 'NN')]\n","[('sweater', 'NN')]\n","[('fit', 'NN')]\n","[('her', 'PRP$')]\n","[('perfectly', 'RB')]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"metadata":{"id":"Q6qI1wcRNO5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["documentA = 'Jupiter is the largest Planet'\n","documentB = 'Mars is the fourth planet from the Sun'\n","     "],"metadata":{"id":"GpScpQozNSd2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","bagOfWordsA = documentA.split(' ')\n","bagOfWordsB = documentB.split(' ')"],"metadata":{"id":"A0PshOz0VH8s","executionInfo":{"status":"ok","timestamp":1682575758442,"user_tz":-330,"elapsed":441,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"],"metadata":{"id":"6TdJTgQeVMHC","executionInfo":{"status":"ok","timestamp":1682575774650,"user_tz":-330,"elapsed":408,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["uniqueWords"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0F38PoG3VPQR","executionInfo":{"status":"ok","timestamp":1682575788340,"user_tz":-330,"elapsed":420,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}},"outputId":"18e01f82-1ae8-40d6-ff43-05a971fdd98b"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Jupiter',\n"," 'Mars',\n"," 'Planet',\n"," 'Sun',\n"," 'fourth',\n"," 'from',\n"," 'is',\n"," 'largest',\n"," 'planet',\n"," 'the'}"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["numOfWordsA = dict.fromkeys(uniqueWords, 0)\n","for word in bagOfWordsA:\n"," numOfWordsA[word] += 1"],"metadata":{"id":"W8i7rwxlVSEh","executionInfo":{"status":"ok","timestamp":1682575820367,"user_tz":-330,"elapsed":415,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["\n","numOfWordsA"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51bw_oc9Vbnn","executionInfo":{"status":"ok","timestamp":1682575840078,"user_tz":-330,"elapsed":1081,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}},"outputId":"edc0899f-2271-4322-ac49-3cf23d3a9578"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Planet': 1,\n"," 'planet': 0,\n"," 'fourth': 0,\n"," 'Jupiter': 1,\n"," 'the': 1,\n"," 'Sun': 0,\n"," 'is': 1,\n"," 'largest': 1,\n"," 'from': 0,\n"," 'Mars': 0}"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["numOfWordsB = dict.fromkeys(uniqueWords, 0)\n","for word in bagOfWordsB:\n","  numOfWordsB[word] += 1\n","     "],"metadata":{"id":"aqq9uu2IVe2O","executionInfo":{"status":"ok","timestamp":1682575852074,"user_tz":-330,"elapsed":451,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["def computeTF(wordDict, bagOfWords):\n"," tfDict = {}\n"," bagOfWordsCount = len(bagOfWords)\n"," for word, count in wordDict.items():\n","   tfDict[word] = count / float(bagOfWordsCount)\n"," return tfDict\n"],"metadata":{"id":"6Tn72Dk3ViDI","executionInfo":{"status":"ok","timestamp":1682575865265,"user_tz":-330,"elapsed":991,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["tfA = computeTF(numOfWordsA, bagOfWordsA)\n","tfB = computeTF(numOfWordsB, bagOfWordsB)"],"metadata":{"id":"zHpC6z2SVlN1","executionInfo":{"status":"ok","timestamp":1682575877596,"user_tz":-330,"elapsed":438,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["tfA "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muDiGtl4VnrG","executionInfo":{"status":"ok","timestamp":1682575887522,"user_tz":-330,"elapsed":441,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}},"outputId":"320693e0-2818-458a-a10a-3a01df2c1d7e"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Planet': 0.2,\n"," 'planet': 0.0,\n"," 'fourth': 0.0,\n"," 'Jupiter': 0.2,\n"," 'the': 0.2,\n"," 'Sun': 0.0,\n"," 'is': 0.2,\n"," 'largest': 0.2,\n"," 'from': 0.0,\n"," 'Mars': 0.0}"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["def computeIDF(documents):\n"," import math\n"," N = len(documents)\n"," idfDict = dict.fromkeys(documents[0].keys(), 0)\n"," for document in documents:\n","  for word, val in document.items():\n","   if val > 0:\n","    idfDict[word] += 1\n"," for word, val in idfDict.items():\n","  idfDict[word] = math.log(N / float(val))\n"," return idfDict"],"metadata":{"id":"7Agimd8zVr8y","executionInfo":{"status":"ok","timestamp":1682575906241,"user_tz":-330,"elapsed":1108,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["idfs = computeIDF([numOfWordsA, numOfWordsB])\n","idfs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bYNxlPwTVu3Y","executionInfo":{"status":"ok","timestamp":1682575916858,"user_tz":-330,"elapsed":430,"user":{"displayName":"Rutuja S","userId":"10184541206491427798"}},"outputId":"d06b52ad-4569-43fa-9f64-1facd768f382"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Planet': 0.6931471805599453,\n"," 'planet': 0.6931471805599453,\n"," 'fourth': 0.6931471805599453,\n"," 'Jupiter': 0.6931471805599453,\n"," 'the': 0.0,\n"," 'Sun': 0.6931471805599453,\n"," 'is': 0.0,\n"," 'largest': 0.6931471805599453,\n"," 'from': 0.6931471805599453,\n"," 'Mars': 0.6931471805599453}"]},"metadata":{},"execution_count":36}]}]}